# The four fundamental equations behind backpropagation

Backpropagation is about understanding how changing the weights and biases in a network changes the cost function. Ultimately, this means computing the partial derivatives 
<img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20w%5E%7Bl%7D_%7Bjk%7D"/>  and  <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20b%5E%7Bl%7D_%7Bj%7D"/>.  But to compute those, we first introduce an intermediate quantity, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> which we call the *error* in the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20j%5E%7Bth%7D"/> neuron in the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20l%5E%7Bth%7D"/> layer.
Backpropagation will give us a procedure to compute the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/>, and then will relate <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> to <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20w%5E%7Bl%7D_%7Bjk%7D"/>  and  <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20b%5E%7Bl%7D_%7Bj%7D"/>.

To understand how the error is defined, imagine there is a demon in our neural network:

<img src="http://neuralnetworksanddeeplearning.com/images/tikz19.png"/>

The demon sits at the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> neuron in layer l. As the input to the neuron comes in, the demon messes with the neuron's operation. It adds a little change <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> to the neuron's weighted input, so that instead of outputting <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%20%28z%5E%7Bl%7D_%7Bj%7D%29"/>, the neuron instead outputs <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%20%28z%5E%7Bl%7D_%7Bj%7D%20&plus;%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D%29"/>. This change propagates through later layers in the network, finally causing the overall cost to change by an amount <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/>.

Now, this demon is a good demon, and is trying to help you improve the cost, i.e., they're trying to find a <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> which makes the cost smaller. Suppose <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> has a large value (either positive or negative). Then the demon can lower the cost quite a bit by choosing <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> to have the opposite sign to <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/>.
By contrast, if <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> is close to zero, then the demon can't improve the cost much at all by perturbing the weighted input <img src="https://latex.codecogs.com/png.latex?%5Csmall%20z%5E%7Bl%7D_%7Bj%7D"/>. So far as the demon can tell, the neuron is already pretty near optimal\*.  And so there's a heuristic sense in which <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> is a measure of the error in the neuron.

> \* This is only the case for small changes <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/>, of course. We'll assume that the demon is constrained to make such small changes.

Motivated by this story, we define the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> of neuron j in layer l by

<img src="https://latex.codecogs.com/png.latex?%5Cdelta_%7Bj%7D%5E%7Bl%7D%20%5Cequiv%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z_%7Bj%7D%5E%7Bl%7D%7D"/>

As per our usual conventions,
