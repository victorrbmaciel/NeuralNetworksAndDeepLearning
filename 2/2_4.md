# The four fundamental equations behind backpropagation

Backpropagation is about understanding how changing the weights and biases in a network changes the cost function. Ultimately, this means computing the partial derivatives 
<img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20w%5E%7Bl%7D_%7Bjk%7D"/>  and  <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20b%5E%7Bl%7D_%7Bj%7D"/>.  But to compute those, we first introduce an intermediate quantity, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> which we call the *error* in the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20j%5E%7Bth%7D"/> neuron in the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20l%5E%7Bth%7D"/> layer.
Backpropagation will give us a procedure to compute the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/>, and then will relate <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> to <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20w%5E%7Bl%7D_%7Bjk%7D"/>  and  <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20b%5E%7Bl%7D_%7Bj%7D"/>.

To understand how the error is defined, imagine there is a demon in our neural network:

<img align="center" src="http://neuralnetworksanddeeplearning.com/images/tikz19.png"/>

The demon sits at the <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> neuron in layer l. As the input to the neuron comes in, the demon messes with the neuron's operation. It adds a little change <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> to the neuron's weighted input, so that instead of outputting <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%20%28z%5E%7Bl%7D_%7Bj%7D%29"/>, the neuron instead outputs <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Csigma%20%28z%5E%7Bl%7D_%7Bj%7D%20&plus;%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D%29"/>. This change propagates through later layers in the network, finally causing the overall cost to change by an amount <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/>.

Now, this demon is a good demon, and is trying to help you improve the cost, i.e., they're trying to find a <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> which makes the cost smaller. Suppose <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> has a large value (either positive or negative). Then the demon can lower the cost quite a bit by choosing <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/> to have the opposite sign to <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/>.
By contrast, if <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> is close to zero, then the demon can't improve the cost much at all by perturbing the weighted input <img src="https://latex.codecogs.com/png.latex?%5Csmall%20z%5E%7Bl%7D_%7Bj%7D"/>. So far as the demon can tell, the neuron is already pretty near optimal\*.  And so there's a heuristic sense in which <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7Bl%7D_%7Bj%7D%7D"/> is a measure of the error in the neuron.

> \* This is only the case for small changes <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5CDelta%20z%5E%7Bl%7D_%7Bj%7D"/>, of course. We'll assume that the demon is constrained to make such small changes.

Motivated by this story, we define the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D"/> of neuron j in layer l by

<img align="center" src="https://latex.codecogs.com/png.latex?%5Cdelta_%7Bj%7D%5E%7Bl%7D%20%5Cequiv%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z_%7Bj%7D%5E%7Bl%7D%7D"/>

As per our usual conventions, we use <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta^l"/> to denote the vector of errors associated with layer l. Backpropagation will give us a way of computing <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_l"/> for every layer, and then relating those errors to the quantities of real interest, <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20w%5E%7Bl%7D_%7Bjk%7D"/> and <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cpartial%20C/%5Cpartial%20b%5E%7Bl%7D_%7Bj%7D"/>.

You might wonder why the demon is changing the weighted input <img src="https://latex.codecogs.com/png.latex?%5Csmall%20z%5E%7Bl%7D_%7Bj%7D"/>. Surely it'd be more natural to imagine the demon changing the output activation <img src="https://latex.codecogs.com/png.latex?%5Csmall%20a_%7Bj%7D%5E%7Bl%7D"/>, with the result that we'd be using <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20a_%7Bj%7D%5E%7Bl%7D%7D"/> as our measure of error. In fact, if you do this things work out quite similarly to the discussion below. But it turns out to make the presentation of backpropagation a little more algebraically complicated. So we'll stick with <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7Bl%7D%20%3D%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z_%7Bj%7D%5E%7Bl%7D%7D"/> as our measure of error.\*

> \* In classification problems like MNIST the term "error" is sometimes used to mean the classification failure rate. E.g., if the neural net correctly classifies 96.0 percent of the digits, then the error is 4.0 percent. Obviously, this has quite a different meaning from our Î´ vectors. In practice, you shouldn't have trouble telling which meaning is intended in any given usage.

**Plan of attack:** Backpropagation is based around four fundamental equations. Together, those equations give us a way of computing both the error <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta^l"/> and the gradient of the cost function. I state the four equations below. Be warned, though: you shouldn't expect to instantaneously assimilate the equations. Such an expectation will lead to disappointment. In fact, the backpropagation equations are so rich that understanding them well requires considerable time and patience as you gradually delve deeper into the equations. The good news is that such patience is repaid many times over. And so the discussion in this section is merely a beginning, helping you on the way to a thorough understanding of the equations.

Here's a preview of the ways we'll delve more deeply into the equations later in the chapter: [I'll give a short proof of the equations](https://github.com/ras-ufcg/NeuralNetworksAndDeepLearning/blob/main/2/2_5.md), which helps explain why they are true; we'll [restate the equations](https://github.com/ras-ufcg/NeuralNetworksAndDeepLearning/blob/main/2/2_6.md) in algorithmic form as pseudocode, and [see how](https://github.com/ras-ufcg/NeuralNetworksAndDeepLearning/blob/main/2/2_7.md) the pseudocode can be implemented as real, running Python code; and, in the [final section of the chapter](https://github.com/ras-ufcg/NeuralNetworksAndDeepLearning/blob/main/2/2_9.md), we'll develop an intuitive picture of what the backpropagation equations mean, and how someone might discover them from scratch. Along the way we'll return repeatedly to the four fundamental equations, and as you deepen your understanding those equations will come to seem comfortable and, perhaps, even beautiful and natural.

**An equation for the error in the output layer,** <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta^L"/>: The components of <img src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta^L"/> are given by

<img align="center" src="https://latex.codecogs.com/png.latex?%5Csmall%20%5Cdelta_%7Bj%7D%5E%7BL%7D%20%3D%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20a_%7Bj%7D%5E%7BL%7D%7D%5Csigma%27%28z_%7Bj%7D%5E%7BL%7D%29"/>

This is a very natural expression...
