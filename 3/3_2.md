# Overfitting and regularization

## Regularization
Increasing the amount of training data is one way of reducing overfitting. Are there other ways we can reduce the extent to which overfitting occurs? One possible approach is to reduce the size of our network. However, large networks have the potential to be more powerful than small networks, and so this is an option we'd only adopt reluctantly.

Fortunately, there are other techniques which can reduce overfitting, even when we have a fixed network and fixed training data. These are known as _regularization techniques_. In this section I describe one of the most commonly used regularization techniques, a technique sometimes known as _weight decay_ or L2 regularization. The idea of L2 _regularization_ is to add an extra term to the cost function, a term called the regularization term. Here's the regularized cross-entropy:

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141347331-c0284820-ad76-4d0d-a924-e8136bd53f81.png" width="400"/><b>(85)</b>
</p>

The first term is just the usual expression for the cross-entropy. But we've added a second term, namely the sum of the squares of all the weights in the network. This is scaled by a factor λ/2n, where λ>0 is known as the _regularization parameter_, and n is, as usual, the size of our training set. I'll discuss later how λ is chosen. It's also worth noting that the regularization term _doesn't_ include the biases. I'll also come back to that below.

Of course, it's possible to regularize other cost functions, such as the quadratic cost. This can be done in a similar way:

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141347465-ff8a8238-0956-403c-81cf-09a5e835605e.png" width="300"/><b>(86)</b>
</p>

In both cases we can write the regularized cost function as

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141347629-9b616b0b-d182-4842-a566-1863568b094b.png" width="200"/><b>(87)</b>
</p>

where C<sub>0</sub> is the original, unregularized cost function.

Intuitively, the effect of regularization is to make it so the network prefers to learn small weights, all other things being equal. Large weights will only be allowed if they considerably improve the first part of the cost function. Put another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original cost function. The relative importance of the two elements of the compromise depends on the value of λ: when λ is small we prefer to minimize the original cost function, but when λ is large we prefer small weights.

Now, it's really not at all obvious why making this kind of compromise should help reduce overfitting! But it turns out that it does. We'll address the question of why it helps in the next section. But first, let's work through an example showing that regularization really does reduce overfitting.

To construct such an example, we first need to figure out how to apply our stochastic gradient descent learning algorithm in a regularized neural network. In particular, we need to know how to compute the partial derivatives ∂C/∂w and ∂C/∂b for all the weights and biases in the network. Taking the partial derivatives of Equation (87) gives

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141347725-b7e9c613-12bd-4078-8946-a0c7193a78a4.png" width="150"/><b>(88)</b>
</p>
<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141347740-9a549e0c-3101-44a5-b4ad-199c5b4177c7.png" width="100"/><b>(89)</b>
</p>

The ∂C0/∂w and ∂C0/∂b terms can be computed using backpropagation, as described in the last chapter. And so we see that it's easy to compute the gradient of the regularized cost function: just use backpropagation, as usual, and then add (λ/n)w to the partial derivative of all the weight terms. The partial derivatives with respect to the biases are unchanged, and so the gradient descent learning rule for the biases doesn't change from the usual rule:

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141347836-c480789d-f2f1-458e-94b2-32fa5c057c94.png" width="150"/><b>(90)</b>
</p>

The learning rule for the weights becomes:

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141347990-bd29c862-99f1-48ca-9bb7-f5312794f34a.png" width="200"/><b>(91)</b>
</p>
<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141348012-52a5d9ab-5aac-4620-b2c2-132c81ba6957.png" width="200"/><b>(92)</b>
</p>

This is exactly the same as the usual gradient descent learning rule, except we first rescale the weight w by a factor 1−(ηλ)/n. This rescaling is sometimes referred to as _weight decay_, since it makes the weights smaller. At first glance it looks as though this means the weights are being driven unstoppably toward zero. But that's not right, since the other term may lead the weights to increase, if so doing causes a decrease in the unregularized cost function.

Okay, that's how gradient descent works. What about stochastic gradient descent? Well, just as in unregularized stochastic gradient descent, we can estimate ∂C0/∂w by averaging over a mini-batch of m training examples. Thus the regularized learning rule for stochastic gradient descent becomes (c.f. Equation (20))

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141348168-a86d17b0-2fa3-4c10-a8c6-415254e73611.png" width="250"/><b>(93)</b>
</p>

where the sum is over training examples x in the mini-batch, and C<sub>x</sub> is the (unregularized) cost for each training example. This is exactly the same as the usual rule for stochastic gradient descent, except for the 1−(ηλ)/n weight decay factor. Finally, and for completeness, let me state the regularized learning rule for the biases. This is, of course, exactly the same as in the unregularized case (c.f. Equation (21)),

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141348242-a7f41eb8-fce6-40a5-90ce-1fd434b1b74b.png" width="150"/><b>(94)</b>
</p>

where the sum is over training examples x in the mini-batch.

Let's see how regularization changes the performance of our neural network. We'll use a network with 30 hidden neurons, a mini-batch size of 10, a learning rate of 0.5, and the cross-entropy cost function. However, this time we'll use a regularization parameter of λ=0.1. Note that in the code, we use the variable name `lmbda`, because `lambda` is a reserved word in Python, with an unrelated meaning. I've also used the `test_data` again, not the `validation_data`. Strictly speaking, we should use the `validation_data`, for all the reasons we discussed earlier. But I decided to use the `test_data` because it makes the results more directly comparable with our earlier, unregularized results. You can easily change the code to use the `validation_data` instead, and you'll find that it gives similar results.

```
>>> import mnist_loader 
>>> training_data, validation_data, test_data = \
... mnist_loader.load_data_wrapper() 
>>> import network2 
>>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)
>>> net.large_weight_initializer()
>>> net.SGD(training_data[:1000], 400, 10, 0.5,
... evaluation_data=test_data, lmbda = 0.1,
... monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,
... monitor_training_cost=True, monitor_training_accuracy=True)
```

The cost on the training data decreases over the whole time, much as it did in the earlier, unregularized case (this and the next two graphs were produced with the program `overfitting.py`):

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141348962-5179ba6d-5a15-4ef1-868f-8494b06a82eb.png" width="400"/>
</p>

But this time the accuracy on the `test_data` continues to increase for the entire 400 epochs:

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141348936-143362ec-7ebc-4699-9961-d251a1bc67af.png" width="400"/>
</p>

Clearly, the use of regularization has suppressed overfitting. What's more, the accuracy is considerably higher, with a peak classification accuracy of 87.1 percent, compared to the peak of 82.27 percent obtained in the unregularized case. Indeed, we could almost certainly get considerably better results by continuing to train past 400 epochs. It seems that, empirically, regularization is causing our network to generalize better, and considerably reducing the effects of overfitting.

What happens if we move out of the artificial environment of just having 1,000 training images, and return to the full 50,000 image training set? Of course, we've seen already that overfitting is much less of a problem with the full 50,000 images. Does regularization help any further? Let's keep the hyper-parameters the same as before - 30 epochs, learning rate 0.5, mini-batch size of 10. However, we need to modify the regularization parameter. The reason is because the size n of the training set has changed from n=1,000 to n=50,000, and this changes the weight decay factor 1−(ηλ)/n. If we continued to use λ=0.1 that would mean much less weight decay, and thus much less of a regularization effect. We compensate by changing to λ=5.0.

Okay, let's train our network, stopping first to re-initialize the weights:

```
>>> net.large_weight_initializer()
>>> net.SGD(training_data, 30, 10, 0.5,
... evaluation_data=test_data, lmbda = 5.0,
... monitor_evaluation_accuracy=True, monitor_training_accuracy=True)
```

We obtain the results:

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/141348910-60106cfb-1202-45d5-ba24-8de1d233946a.png" width="400"/>
</p>

There's lots of good news here. First, our classification accuracy on the test data is up, from 95.49 percent when running unregularized, to 96.49 percent. That's a big improvement. Second, we can see that the gap between results on the training and test data is much narrower than before, running at under a percent. That's still a significant gap, but we've obviously made substantial progress reducing overfitting.

Finally, let's see what test classification accuracy we get when we use 100 hidden neurons and a regularization parameter of λ=5.0. I won't go through a detailed analysis of overfitting here, this is purely for fun, just to see how high an accuracy we can get when we use our new tricks: the cross-entropy cost function and L2 regularization.

```
>>> net = network2.Network([784, 100, 10], cost=network2.CrossEntropyCost)
>>> net.large_weight_initializer()
>>> net.SGD(training_data, 30, 10, 0.5, lmbda=5.0,
... evaluation_data=validation_data,
... monitor_evaluation_accuracy=True)
```

The final result is a classification accuracy of 97.92 percent on the validation data. That's a big jump from the 30 hidden neuron case. In fact, tuning just a little more, to run for 60 epochs at η=0.1 and λ=5.0 we break the 98 percent barrier, achieving 98.04 percent classification accuracy on the validation data. Not bad for what turns out to be 152 lines of code!

I've described regularization as a way to reduce overfitting and to increase classification accuracies. In fact, that's not the only benefit. Empirically, when doing multiple runs of our MNIST networks, but with different (random) weight initializations, I've found that the unregularized runs will occasionally get "stuck", apparently caught in local minima of the cost function. The result is that different runs sometimes provide quite different results. By contrast, the regularized runs have provided much more easily replicable results.

Why is this going on? Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long. I believe this phenomenon is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.

## Why does regularization help reduce overfitting?

We've seen empirically that regularization helps reduce overfitting. That's encouraging but, unfortunately, it's not obvious why regularization helps! A standard story people tell to explain what's going on is along the following lines: smaller weights are, in some sense, lower complexity, and so provide a simpler and more powerful explanation for the data, and should thus be preferred. That's a pretty terse story, though, and contains several elements that perhaps seem dubious or mystifying. Let's unpack the story and examine it critically. To do that, let's suppose we have a simple data set for which we wish to build a model:

<p align="center">
  <img src="https://i.ibb.co/G3t28sk/img-book01.png" width="400"/>
</p>

Implicitly, we're studying some real-world phenomenon here, with x and y representing real-world data. Our goal is to build a model which lets us predict y as a function of x. We could try using neural networks to build such a model, but I'm going to do something even simpler: I'll try to model y as a polynomial in x. I'm doing this instead of using neural nets because using polynomials will make things particularly transparent. Once we've understood the polynomial case, we'll translate to neural networks. Now, there are ten points in the graph above, which means we can find a unique 9th-order polynomial **(EDITAR AQUI)** y=a0x9+a1x8+…+a9 which fits the data exactly. Here's the graph of that polynomial*

> I won't show the coefficients explicitly, although they are easy to find using a routine such as Numpy's `polyfit`. You can view the exact form of the polynomial in the source code for the graph if you're curious. It's the function p(x) defined starting on line 14 of the program which produces the graph.

